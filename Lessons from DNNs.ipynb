{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential \n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# dataset : train : 45,000 (32x32x3) /dev : 5,000 / test :10,000 (32x32x3)\n",
    "\n",
    "(X_traindev, Y_traindev), (X_test, Y_test) = cifar10.load_data() \n",
    "\n",
    "X_traindev = X_traindev.reshape(50000, 3072)     \n",
    "X_test = X_test.reshape(10000, 3072)\n",
    "X_traindev = X_traindev.astype('float32')     \n",
    "X_test = X_test.astype('float32')     \n",
    "X_traindev /= 255  \n",
    "X_test /= 255\n",
    "X_train = X_traindev[:45000]\n",
    "Y_train = Y_traindev[:45000]\n",
    "X_dev = X_traindev[45000:50000]\n",
    "Y_dev = Y_traindev[45000:50000]\n",
    "classes = 10\n",
    "Y_train_onehot = np_utils.to_categorical(Y_train, classes)     \n",
    "Y_dev_onehot = np_utils.to_categorical(Y_dev, classes)\n",
    "Y_test_onehot = np_utils.to_categorical(Y_test, classes)\n",
    "print(\"New X_train shape: {} \\nNew Y_train shape:{}\".format(X_train.shape, Y_train.shape))\n",
    "\n",
    "\n",
    "# Building model \n",
    "\n",
    "alter_model = Sequential()\n",
    "alter_model.add(layers.Dropout(0.2))\n",
    "alter_model.add(Dense(400, activation='relu', input_dim=3072, kernel_regularizer = regularizers.l2(l=0.001)))\n",
    "alter_model.add(layers.Dropout(0.2))\n",
    "alter_model.add(Dense(200, activation='relu',kernel_regularizer = regularizers.l2(l=0.001)))\n",
    "alter_model.add(layers.Dropout(0.2))\n",
    "alter_model.add(Dense(100, activation='relu',kernel_regularizer = regularizers.l2(l=0.001)))\n",
    "alter_model.add(layers.Dropout(0.2))\n",
    "alter_model.add(Dense(50, activation='relu',kernel_regularizer = regularizers.l2(l=0.001)) )\n",
    "alter_model.add(layers.Dropout(0.2))\n",
    "alter_model.add(Dense(20, activation='relu',kernel_regularizer = regularizers.l2(l=0.001)))\n",
    "alter_model.add(Dense(classes, activation='softmax')) \n",
    "alter_model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='sgd')\n",
    "\n",
    "# Model training & Save the history \n",
    "\n",
    "small_model_history = alter_model.fit(X_train, Y_train_onehot, epochs=1000, batch_size = 512, \n",
    "                                      validation_data=(X_dev,Y_dev_onehot), verbose=2)\n",
    "model_history_df = pd.DataFrame(small_model_history.history, columns=['accuracy','val_accuracy','loss','val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_history_df.to_csv('model_history.csv')\n",
    "save_dir = '/home/ubuntu/HJ Bae/CIFAR10'\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, '1000epoch')\n",
    "alter_model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize history\n",
    "\n",
    "acc_history = model_history_df['accuracy']\n",
    "val_acc_history = model_history_df['val_accuracy']\n",
    "loss_history = model_history_df['loss']\n",
    "val_loss_history = model_history_df['val_loss']\n",
    "\n",
    "# accuracy\n",
    "\n",
    "plt.plot(acc_history)\n",
    "plt.plot(val_acc_history)\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','dev'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# loss \n",
    "\n",
    "plt.plot(loss_history)\n",
    "plt.plot(val_loss_history)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','dev'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scenario 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_output = K.function([model.input], \n",
    "                    [model.layers[-1].output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputs of the truck units while presenting 1,000 test images of each of N unit \n",
    "\n",
    "airplane_output = pd.DataFrame(softmax_output([X_test_airplane])[0])[9]\n",
    "automobile_output = pd.DataFrame(softmax_output([X_test_automobile])[0])[9]\n",
    "bird_output = pd.DataFrame(softmax_output([X_test_bird])[0])[9]\n",
    "cat_output = pd.DataFrame(softmax_output([X_test_cat])[0])[9]\n",
    "deer_output = pd.DataFrame(softmax_output([X_test_deer])[0])[9]\n",
    "dog_output = pd.DataFrame(softmax_output([X_test_dog])[0])[9]\n",
    "frog_output = pd.DataFrame(softmax_output([X_test_frog])[0])[9]\n",
    "horse_output = pd.DataFrame(softmax_output([X_test_horse])[0])[9]\n",
    "ship_output = pd.DataFrame(softmax_output([X_test_ship])[0])[9]\n",
    "truck_output = pd.DataFrame(softmax_output([X_test_truck])[0])[9]\n",
    "\n",
    "airplane_mean = np.mean(airplane_output)\n",
    "automobile_mean = np.mean(automobile_output)\n",
    "bird_mean = np.mean(bird_output)\n",
    "cat_mean = np.mean(cat_output)\n",
    "deer_mean = np.mean(deer_output)\n",
    "dog_mean = np.mean(dog_output)\n",
    "frog_mean = np.mean(frog_output)\n",
    "horse_mean = np.mean(horse_output)\n",
    "ship_mean = np.mean(ship_output)\n",
    "truck_mean = np.mean(truck_output)\n",
    "\n",
    "mean = [airplane_mean, automobile_mean, bird_mean, cat_mean, deer_mean, dog_mean, frog_mean, horse_mean, ship_mean, truck_mean]\n",
    "\n",
    "y= mean\n",
    "x = np.arange(len(y))\n",
    "xlabel=['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
    "plt.rcParams[\"figure.figsize\"] =(10,4)\n",
    "plt.title(\"Selectivity Profile_Truck unit\")\n",
    "plt.bar(x,y)\n",
    "plt.xticks(x, xlabel)\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Activity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scenario 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential \n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import PIL\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\dywl1\\\\Desktop\\새 폴더\\\\ANN BNN\\\\keras\\\\CIFAR10\\\\Part3(로고)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\dywl1\\\\Desktop\\\\새 폴더\\\\ANN BNN\\\\keras\\\\CIFAR10\\\\Part3(로고)\\\\Raw_ver2\\\\images\\\\train\\\\Cat')\n",
    "paste_cat_train=[]\n",
    "for filename in glob.glob('*0.8paste.png'):\n",
    "    img = load_img(filename)\n",
    "    x = img_to_array(img)\n",
    "    x = x.flatten()\n",
    "    x /= 255\n",
    "    paste_cat_train.append(x)\n",
    "    \n",
    "paste_cat_td = np.asarray(paste_cat_train) # 5000x3072\n",
    "paste_cat_test = np.asarray(paste_cat_test) #1000x3072\n",
    "paste_dog_td = np.asarray(paste_dog_train)\n",
    "paste_dog_test = np.asarray(paste_dog_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loading \n",
    "(X_traindev, Y_traindev), (X_test, Y_test) = cifar10.load_data() \n",
    "\n",
    "X_traindev = X_traindev.reshape(50000, 3072)     \n",
    "X_test = X_test.reshape(10000, 3072)\n",
    "X_traindev = X_traindev.astype('float32')     \n",
    "X_test = X_test.astype('float32')     \n",
    "X_traindev /= 255  \n",
    "X_test /= 255\n",
    "\n",
    "classes = 10\n",
    "Y_traindev_onehot = np_utils.to_categorical(Y_traindev, classes)     \n",
    "Y_test_onehot = np_utils.to_categorical(Y_test, classes)\n",
    "\n",
    "cat_td = np.where(Y_traindev==3)[0] #5000\n",
    "dog_td = np.where(Y_traindev==5)[0]\n",
    "\n",
    "\n",
    "#전체 train/dev/test set에서 cat,dog만 paste file로 replace\n",
    "j=0\n",
    "l=0\n",
    "for i in range(len(X_traindev)):\n",
    "    if i in cat_td:\n",
    "        X_traindev[i] = paste_cat_td[j]\n",
    "        j += 1\n",
    "    if i in dog_td:\n",
    "        X_traindev[i] = paste_dog_td[l]\n",
    "        l += 1\n",
    "j=0\n",
    "l=0        \n",
    "for i in range(len(X_test)):\n",
    "    if i in cat_test:\n",
    "        X_test[i] = paste_cat_test[j]\n",
    "        j +=1\n",
    "    if i in dog_test:\n",
    "        X_test[i] = paste_dog_test[l]\n",
    "        l +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train/dev/test split \n",
    "\n",
    "X_train, X_dev, Y_train, Y_dev = train_test_split(X_traindev, Y_traindev, test_size=0.1, random_state=123)\n",
    "classes = 10\n",
    "Y_train_onehot = np_utils.to_categorical(Y_train, classes)     \n",
    "Y_dev_onehot = np_utils.to_categorical(Y_dev, classes)   \n",
    "Y_test_onehot = np_utils.to_categorical(Y_test, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_cat = X_test[cat_test] \n",
    "X_test_dog = X_test[dog_test]\n",
    "Y_test_cat = Y_test[cat_test]\n",
    "Y_test_dog = Y_test[dog_test]\n",
    "Y_test_cat_onehot = np_utils.to_categorical(Y_test_cat, classes)\n",
    "Y_test_dog_onehot = np_utils.to_categorical(Y_test_dog, classes)\n",
    "\n",
    "airplane_test = np.where(Y_test==0)[0]\n",
    "X_test_airplane = X_test[airplane_test]\n",
    "Y_test_airplane = Y_test[airplane_test]\n",
    "Y_test_airplane_onehot = np_utils.to_categorical(Y_test_airplane, classes)\n",
    "\n",
    "\n",
    "print(model.evaluate(X_test, Y_test_onehot)) #52.67%\n",
    "test = np.concatenate((paste_cat_test, paste_dog_test), axis=0)\n",
    "test_answer = np.concatenate((Y_test_cat_onehot, Y_test_dog_onehot), axis=0)\n",
    "print(model.evaluate(test, test_answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstructing the receptive field vector \n",
    "\n",
    "from keras import layers\n",
    "# (layer0,2,4,6,8: dropout layer), layer1,3,5,7,9: hidden layer, 10: output layer)\n",
    "\n",
    "weight =[]\n",
    "bias=[]\n",
    "for i in range(len(model.layers)):\n",
    "    if i %2 != 0 :\n",
    "        W = model.layers[i].get_weights()[0]\n",
    "        B = model.layers[i].get_weights()[1] \n",
    "        weight.append(W) \n",
    "        bias.append(B)\n",
    "    if i == 10:\n",
    "        W = model.layers[i].get_weights()[0]\n",
    "        B = model.layers[i].get_weights()[1]\n",
    "        weight.append(W)\n",
    "        bias.append(B)          \n",
    "weight = np.asarray(weight)\n",
    "bias = np.asarray(bias)\n",
    "\n",
    "# Inverse of the activation function \n",
    "def ReLu_inv(A):\n",
    "    for i in range(len(A)):\n",
    "        if A[i] >=0 :\n",
    "            A[i] = A[i]\n",
    "        else :\n",
    "            A[i] = 0\n",
    "    return A\n",
    "\n",
    "classes = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck' ]\n",
    "\n",
    "# onehot vectors of each class (dict['class'])\n",
    "dict={}\n",
    "for i, x in enumerate(classes):\n",
    "    dict[x] = np.zeros((10,))\n",
    "    dict[x][i]=1\n",
    "\n",
    "\n",
    "# Reconstruct the RF vector \n",
    "rf_dict_array={}\n",
    "rf_dict_img={}    \n",
    "for i, x in enumerate(classes):\n",
    "    A5 = np.dot(weight[-1], dict[x]) #마지막 layer에서는 onehot으로 바꿨기때문에, bias빼주는 연산 생략\n",
    "    A4 = np.dot(weight[-2], ReLu_inv(A5)) \n",
    "    A3 = np.dot(weight[-3], ReLu_inv(A4)) \n",
    "    A2 = np.dot(weight[-4], ReLu_inv(A3)) \n",
    "    A1 = np.dot(weight[-5], ReLu_inv(A2))\n",
    "    A0 = np.dot(weight[-6], ReLu_inv(A1))\n",
    "    rf_dict_array[x] = A0\n",
    "    rf_dict_img[x] = array_to_img(A0.reshape((32,32,3)))    \n",
    "\n",
    "\n",
    "for i,x in enumerate(classes):\n",
    "    print(model.predict_classes(np.array([rf_dict_array[x],])))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate on another dataset(MNIST)\n",
    "\n",
    "model = load_model('C:\\\\Users\\\\dywl1\\\\FCN_MNIST') \n",
    "dict={}\n",
    "classes=[0,1,2,3,4,5,6,7,8,9]\n",
    "for i, x in enumerate(classes):\n",
    "    dict[x] = np.zeros((10,)).astype(int)\n",
    "    dict[x][i]=1\n",
    "    dict[x].reshape((1,10))\n",
    "\n",
    "\n",
    "weight =[]\n",
    "for i in range(len(model.layers)):\n",
    "        W = model.layers[i].get_weights()[0]\n",
    "        weight.append(W)     \n",
    "weight = np.asarray(weight)\n",
    "\n",
    "\n",
    "# Inverse of the activation function \n",
    "def ReLu_inv(A):\n",
    "    for i in range(len(A)):\n",
    "        if A[i] >=0 :\n",
    "            A[i] = A[i]\n",
    "        else :\n",
    "            A[i] = 0\n",
    "    return A\n",
    "\n",
    "rf_dict_array={}\n",
    "for i, x in enumerate(classes):\n",
    "    A2 = np.dot(weight[-1], dict[x]) \n",
    "    A1 = np.dot(weight[-2], ReLu_inv(A2)) \n",
    "    A0 = np.dot(weight[-3], ReLu_inv(A1))\n",
    "    rf_dict_array[x] = A0\n",
    "\n",
    "# Predict the class from the reconstructed RF vector \n",
    "\n",
    "for i,x in enumerate(classes):\n",
    "    print(model.predict_classes(np.array([rf_dict_array[x],])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scenario 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  \n",
    "%matplotlib inline\n",
    "from sklearn.svm import SVC  \n",
    "from sklearn import metrics \n",
    "\n",
    "import numpy as np\n",
    "cat = np.where(Y_test==3)[0]\n",
    "dog = np.where(Y_test==5)[0]\n",
    "\n",
    "X_test_cat = X_test[cat] # 1000개의 3072size(32x32x3) vector\n",
    "X_test_dog = X_test[dog]\n",
    "Y_test_cat = Y_test[cat]\n",
    "Y_test_dog = Y_test[dog]\n",
    "Y_test_cat_onehot = np_utils.to_categorical(Y_test_cat, classes)\n",
    "Y_test_dog_onehot = np_utils.to_categorical(Y_test_dog, classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_output = K.function([dropout_model.input], \n",
    "                    [dropout_model.layers[-1].output])\n",
    "\n",
    "X_test_cat_output = softmax_output([X_test_cat])[0] # 1000x10\n",
    "X_test_dog_output = softmax_output([X_test_dog])[0] # 1000x10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear SVM \n",
    "\n",
    "svm_train = np.concatenate((X_test_truck_output[:800],X_test_ship_output[:800]),axis=0) # 1600x10 (800개씩)\n",
    "svm_train_answer = np.concatenate((Y_test_truck[:800],Y_test_ship[:800]),axis=0)\n",
    "svm_test = np.concatenate((X_test_truck_output[800:1000],X_test_ship_output[800:1000]),axis=0) # 400x10 (200개씩)\n",
    "svm_test_answer = np.concatenate((Y_test_truck[800:1000],Y_test_ship[800:1000]),axis=0)\n",
    "\n",
    "svclassifier = SVC(kernel='linear')  \n",
    "svclassifier.fit(svm_train, svm_train_answer) \n",
    "y_pred = svclassifier.predict(svm_test)  \n",
    "print(\"Accuracy:\",metrics.accuracy_score(svm_test_answer, y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_test = np.concatenate((X_test_ship[800:1000],X_test_truck[800:1000]),axis=0) # 400x10 (200 each ) -> test set same with the Linear SVM\n",
    "model_test_answer = np.concatenate((Y_test_ship_onehot[800:1000],Y_test_truck_onehot[800:1000]),axis=0)\n",
    "print(model.evaluate(model_test, model_test_answer))\n",
    "\n",
    "Eval_total = model.evaluate(X_test, Y_test_onehot) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scenario 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import os\n",
    "import glob\n",
    "import itertools\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import layers\n",
    "import seaborn as sns\n",
    "from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "from keras.models import load_model \n",
    "\n",
    "model = load_model('FCN_CIFAR10_Alter(500)') \n",
    "units = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get outputs\n",
    "\n",
    "inp = model.input\n",
    "outputs = [layer.output for layer in model.layers] # all layer outputs\n",
    "functions = [K.function([inp, K.learning_phase()], [out]) for out in outputs] #evaluation\n",
    "\n",
    "# test set (100 per class)\n",
    "\n",
    "test=[] \n",
    "os.chdir('/home/ubuntu/HJ Bae/CIFAR10')\n",
    "for filename in glob.glob('**//*.png'):\n",
    "   img = load_img(filename)\n",
    "   x = img_to_array(img)\n",
    "   x = x.flatten()\n",
    "   x /= 255\n",
    "   test.append(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer output function \n",
    "\n",
    "layer_outs = [func([test,1]) for func in functions] # layerwise output for 10 species of dog (learning_phase=train(1))\n",
    "softmax_outs = layer_outs[-1][0] # 10,000(subtype image 1000*10)x10(unit)\n",
    "df_softmax = pd.DataFrame(softmax_outs, columns=units) # 10,000 x 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part4_ver1. model with fine/coarse-grained representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part4_ ver1.\n",
    "\n",
    "# 1. Calculate sd1, sd2 (intrinsic noise level)\n",
    "\n",
    "f_value = 3.44 # alpha=0.05, df1=df2=8 (9-1)   \n",
    "unit_output={}\n",
    "se={}\n",
    "sd_coarse={}\n",
    "sd_fine={}\n",
    "\n",
    "for i, x in enumerate(units):\n",
    "    unit_output[x] = df_softmax.iloc[1000*i:1000*(i+1),i] # activations of each of N unit while presenting 1,000 images of the class of which the unit tuned to \n",
    "\n",
    "for key, value in unit_output.items():\n",
    "    se[key] = np.std(unit_output[key]) # SD(sample means) = standard error of mean\n",
    "    \n",
    "sd_coarse = {key:(value*np.sqrt(9)) for key, value in se.items()} # s.e * root(9)=sd1\n",
    "sd_fine = {key: np.sqrt(np.square(value)/f_value) for key, value in sd_coarse.items()} # sd2 << sd1 (based on F-distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. sampling from each distribution (multivariate normal) \n",
    "\n",
    "df = df_softmax.iloc[5000:6000,:] # 1000(dog) x 10(unit) \n",
    "allNeu_1 = []\n",
    "allNeu_2 = []\n",
    "np.random.seed(0)\n",
    "\n",
    "for i, x in enumerate(units):\n",
    "    sd_c = sd_coarse[x]\n",
    "    sd_f = sd_fine[x]\n",
    "    means = df[x]\n",
    "    cov_1 = np.square(sd_c) * np.identity(1000)\n",
    "    cov_2 = np.square(sd_f) * np.identity(1000)\n",
    "    neuron_1 = np.random.multivariate_normal(mean=means, cov=cov_1, size=9) #(9,1000) withinunit_9 neuron x 1000dog\n",
    "    neuron_2 = np.random.multivariate_normal(mean=means, cov=cov_2, size=9)\n",
    "    allNeu_1.append(neuron_1) # nested list (10 (unit) x ([9,1000]))\n",
    "    allNeu_2.append(neuron_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create empty dataframe (1000(dog) x 90(neuron))\n",
    "    \n",
    "index=[]\n",
    "for i in range(1000):\n",
    "    i +=1\n",
    "    idx = 'dog_'+str(i)\n",
    "    index.append(idx)\n",
    "columns=[]\n",
    "for i in range(90):\n",
    "    i += 1\n",
    "    column='neuron_'+ str(i)\n",
    "    columns.append(column)\n",
    "col = []\n",
    "col.append(units)\n",
    "col.append(columns)\n",
    "col[0]=list(itertools.chain.from_iterable(itertools.repeat(unit,9) for unit in col[0]))\n",
    "df_model1 = pd.DataFrame(index=index, columns=columns)\n",
    "df_model2 = pd.DataFrame(index=index, columns=columns)\n",
    "df_model1.columns = pd.MultiIndex.from_arrays((col[0],col[1])) #col[0]= unit, col[1]= neuron number1-90\n",
    "df_model2.columns = pd.MultiIndex.from_arrays((col[0],col[1]))\n",
    "\n",
    "# 4. Fill df  = 1000(dog) x 90 (individual neuron (9 * 10 unit)\n",
    "\n",
    "for i,j in zip(range(10), range(10)):\n",
    "    df_model1.iloc[:,9*i:9*(i+1)]=allNeu_1[j].T\n",
    "    df_model2.iloc[:,9*i:9*(i+1)]=allNeu_2[j].T\n",
    "    \n",
    "# 5. Pairwise correlation (1000-dimensional vector per neuron -> columnwise correlation -> 90x90)\n",
    "    \n",
    "nc_model1 = df_model1.astype('float64').corr(method='pearson')\n",
    "nc_model2 = df_model2.astype('float64').corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Draw the correlation matrix\n",
    "\n",
    "cmap = sns.diverging_palette(220, 20, sep=20, as_cmap=True)\n",
    "xticks = col[0]\n",
    "keptticks = xticks[::int(len(xticks)/10)]\n",
    "xticks = ['' for y in xticks]\n",
    "xticks[::int(len(xticks)/10)] = keptticks\n",
    "\n",
    "sns.heatmap(nc_model1, vmin=-1, vmax=1, cmap=cmap, linewidth=0, xticklabels=xticks, yticklabels=xticks) #model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(nc_model2, vmin=-1, vmax=1, cmap=cmap, linewidth=0, xticklabels=xticks, yticklabels=xticks) #model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_dogunit1 = sns.heatmap(nc_model1.iloc[45:54, 45:54], vmin=-1, vmax=1, cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_dogunit2 = sns.heatmap(nc_model2.iloc[45:54, 45:54], vmin=-1, vmax=1, cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser1 = pd.Series(pd.DataFrame.abs(nc_model1).values.ravel())\n",
    "ser1 = ser1.drop(np.where(ser1==1)[0]) # remove diagnoal correlation \n",
    "ser2 = pd.Series(pd.DataFrame.abs(nc_model2).values.ravel())\n",
    "ser2 = ser2.drop(np.where(ser2==1)[0]) # remove diagnoal correlation\n",
    "print(ser1.mean())\n",
    "print(ser2.mean()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dog unit \n",
    "ser1 = pd.Series(pd.DataFrame.abs(nc_model1.iloc[45:54, 45:54]).values.ravel())\n",
    "ser1 = ser1.drop(np.where(ser1==1)[0])\n",
    "ser2 = pd.Series(pd.DataFrame.abs(nc_model2.iloc[45:54, 45:54]).values.ravel())\n",
    "ser2 = ser2.drop(np.where(ser2==1)[0])\n",
    "print(ser1.mean())\n",
    "print(ser2.mean()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part4_ver2. model with/without internal rhythm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_softmax.iloc[5000:6000,:10] # 1000(dog) x 10(unit) \n",
    "allNeu_1_ver2 = []\n",
    "allNeu_2_ver2 = []\n",
    "np.random.seed(0)\n",
    "\n",
    "for i, x in enumerate(units):\n",
    "    sd = sd_coarse[x]\n",
    "    model1_output = df[x]\n",
    "    cov = np.square(sd) * np.identity(1000)\n",
    "    neuron_1 = np.random.multivariate_normal(mean=model1_output, cov=cov, size=9) #(9,1000) withinunit_9 neuron x 1000dog\n",
    "    allNeu_1_ver2.append(neuron_1) # nested list (10 (unit) x ([9,1000]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(df['airplane']))\n",
    "print(np.var(df['airplane']))\n",
    "print(sd_coarse['airplane'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. model2 sampling \n",
    "    \n",
    "intup = df.iloc[0::2,:10] + np.random.rand(10) # up_cycle(even idx image) : add randomn number ~ [0,1) to the outputs\n",
    "intdown = df.iloc[1::2,:10] -np.random.rand(10) # down_cycle (odd idx image) : subtract random number ~[0,1) from the outputs\n",
    "intmodel = pd.DataFrame.sort_index(pd.concat([intup, intdown])) \n",
    "\n",
    "for i, x in enumerate(units):\n",
    "    model2_output = intmodel[x]\n",
    "    neuron_2 = np.random.multivariate_normal(mean=model2_output, cov=cov, size=9)\n",
    "    allNeu_2_ver2.append(neuron_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. create empty dataframe (1000(dog) x 90(neuron))\n",
    "    \n",
    "index=[]\n",
    "for i in range(1000):\n",
    "    i +=1\n",
    "    idx = 'dog_'+str(i)\n",
    "    index.append(idx)\n",
    "columns=[]\n",
    "for i in range(90):\n",
    "    i += 1\n",
    "    column='neuron_'+ str(i)\n",
    "    columns.append(column)\n",
    "col = []\n",
    "col.append(units)\n",
    "col.append(columns)\n",
    "col[0]=list(itertools.chain.from_iterable(itertools.repeat(unit,9) for unit in col[0]))\n",
    "\n",
    "df_model1 = pd.DataFrame(index=index, columns=columns)\n",
    "df_model2 = pd.DataFrame(index=index, columns=columns)\n",
    "df_model1.columns = pd.MultiIndex.from_arrays((col[0],col[1])) \n",
    "df_model2.columns = pd.MultiIndex.from_arrays((col[0],col[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Fill df  = 1000(dog) x 90 (neuron) (-> 9 * 10 unit)\n",
    "\n",
    "for i,j in zip(range(10), range(10)):\n",
    "    df_model1.iloc[:,9*i:9*(i+1)]=allNeu_1_ver2[j].T\n",
    "    df_model2.iloc[:,9*i:9*(i+1)]=allNeu_2_ver2[j].T\n",
    "    \n",
    "# 6. pairwise correlation \n",
    "    \n",
    "nc_model1 = df_model1.astype('float64').corr(method='pearson')\n",
    "nc_model2 = df_model2.astype('float64').corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. correlation matrix\n",
    "\n",
    "cmap = sns.diverging_palette(220, 20, sep=20, as_cmap=True)\n",
    "xticks = col[0]\n",
    "keptticks = xticks[::int(len(xticks)/10)]\n",
    "xticks = ['' for y in xticks]\n",
    "xticks[::int(len(xticks)/10)] = keptticks\n",
    "\n",
    "sns.heatmap(nc_model1, vmin=-1, vmax=1, cmap=cmap, linewidth=0, xticklabels=xticks, yticklabels=xticks) # model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(nc_model2, vmin=-1, vmax=1, cmap=cmap, linewidth=0, xticklabels=xticks, yticklabels=xticks) # model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_dogunit1 = sns.heatmap(nc_model1.iloc[45:54, 45:54], vmin=-1, vmax=1, cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_dogunit2 = sns.heatmap(nc_model2.iloc[45:54, 45:54], vmin=-1, vmax=1, cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total unit \n",
    "ser1 = pd.Series(pd.DataFrame.abs(nc_model1).values.ravel())\n",
    "ser1 = ser1.drop(np.where(ser1==1)[0])\n",
    "ser2 = pd.Series(pd.DataFrame.abs(nc_model2).values.ravel())\n",
    "ser2 = ser2.drop(np.where(ser2==1)[0])\n",
    "\n",
    "print(ser1.mean())\n",
    "print(ser2.mean()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dog unit\n",
    "ser1 = pd.Series(pd.DataFrame.abs(nc_model1.iloc[45:54, 45:54]).values.ravel())\n",
    "ser1 = ser1.drop(np.where(ser1==1)[0])\n",
    "ser2 = pd.Series(pd.DataFrame.abs(nc_model2.iloc[45:54, 45:54]).values.ravel())\n",
    "ser2 = ser2.drop(np.where(ser2==1)[0])\n",
    "print(ser1.mean()) \n",
    "print(ser2.mean()) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p27)",
   "language": "python",
   "name": "conda_tensorflow2_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
